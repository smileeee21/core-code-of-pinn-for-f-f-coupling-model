{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94e2f6a7-1461-4136-a39f-d55a9c65343a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.70621634 0.57009855]\n",
      " [0.91500864 0.91346293]\n",
      " [0.44191336 0.39962539]\n",
      " ...\n",
      " [0.36222247 0.56075248]\n",
      " [0.69595732 0.18451741]\n",
      " [0.59026972 0.43176907]]\n",
      "Training loss (every 50 epochs) at epoch 50 : 8.236182880424062\n",
      "Training loss (every 50 epochs) at epoch 100 : 0.89104511807715847\n",
      "Training loss (every 50 epochs) at epoch 150 : 0.29318074110705072\n",
      "Training loss (every 50 epochs) at epoch 200 : 0.14222797383665073\n",
      "Training loss (every 50 epochs) at epoch 250 : 0.061878948431859829\n",
      "Training loss (every 50 epochs) at epoch 300 : 0.042627981595831692\n",
      "Training loss (every 50 epochs) at epoch 350 : 0.03175740628828029\n",
      "Training loss (every 50 epochs) at epoch 400 : 0.028794788615313996\n",
      "Training loss (every 50 epochs) at epoch 450 : 0.031234050263280643\n",
      "Training loss (every 50 epochs) at epoch 500 : 0.024748962232286089\n",
      "Training loss (every 50 epochs) at epoch 550 : 0.086965777926048121\n",
      "Training loss (every 50 epochs) at epoch 600 : 0.021267533804328428\n",
      "Training loss (every 50 epochs) at epoch 650 : 0.01875544712011367\n",
      "Training loss (every 50 epochs) at epoch 700 : 0.033101476474422076\n",
      "Training loss (every 50 epochs) at epoch 750 : 0.015878533045241533\n",
      "Training loss (every 50 epochs) at epoch 800 : 0.028644422713029555\n",
      "Training loss (every 50 epochs) at epoch 850 : 0.020279763317741176\n",
      "Training loss (every 50 epochs) at epoch 900 : 0.014415231022372774\n",
      "Training loss (every 50 epochs) at epoch 950 : 0.026748188379407695\n",
      "Training loss (every 50 epochs) at epoch 1000 : 0.031709614747646692\n",
      "157/157 [==============================] - 0s 276us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "np.random.seed(521)\n",
    "tf.random.set_seed(1314)\n",
    "pi = tf.constant(np.pi, dtype=tf.float64)\n",
    "class HeatPINN(keras.Sequential):\n",
    "    def __init__(self, Layers, name=None):\n",
    "        super(HeatPINN, self).__init__(name=name)\n",
    "        self.add(keras.Input(shape=(Layers[0],), dtype=tf.float64))\n",
    "        for i in range(1, len(Layers)-1):\n",
    "            self.add(keras.layers.Dense(Layers[i], dtype=tf.float64, activation='tanh'))\n",
    "        self.add(keras.layers.Dense(Layers[-1], dtype=tf.float64, name=\"outputs\"))\n",
    "    \n",
    "    @tf.function\n",
    "    def loss_U(self, X_u_train, u_train):\n",
    "        u = self(X_u_train)\n",
    "        loss_u = tf.reduce_mean( tf.square(u_train - u))\n",
    "        return loss_u\n",
    "    \n",
    "    @tf.function\n",
    "    def loss_PDE(self, X_f_train,X_b_train):\n",
    "        x = X_f_train[:, 0:1]\n",
    "        y = X_f_train[:, 1:2]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch([x, y])\n",
    "            X = tf.concat([x, y], axis=1)\n",
    "            u = self(X)\n",
    "            u_x = tape.gradient(u, x)\n",
    "            u_y = tape.gradient(u, y)\n",
    "            u_xx = tape.gradient(u_x, x)\n",
    "            u_yy = tape.gradient(u_y, y)\n",
    "        del tape\n",
    "        u_i=self(X_b_train)\n",
    "        loss_f1= u_xx + u_yy +2*pi**2*tf.sin(pi*x)*tf.sin(pi*y)\n",
    "        loss_f1 = tf.reduce_mean(tf.square(loss_f1))\n",
    "        loss_f2 =tf.reduce_mean(tf.square(u_i))\n",
    "        loss_f = loss_f1 + loss_f2\n",
    "        return loss_f\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, X_u_train, u_train, X_i_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_U(X_u_train, u_train) + self.loss_PDE(X_u_train,X_i_train)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    def train(self, X_u_train, u_train, X_i_train, epochs=200):\n",
    "        \n",
    "        for epoch in tf.range(1, epochs+1):\n",
    "            loss = self.train_step(X_u_train, u_train, X_i_train)\n",
    "            if epoch % 50 == 0:\n",
    "                tf.print(\"Training loss (every 50 epochs) at epoch\", epoch, \":\", loss)\n",
    "                loss_per50.append(loss)\n",
    "# 定义迪利克雷边界条件热传导方程的参数和边界条件函数\n",
    "u_exact = lambda x, y: np.sin(np.pi * x) * np.sin(np.pi * y)  # 精确解\n",
    "\n",
    "# 生成训练数据\n",
    "loss_per50=[]\n",
    "n_u = 5000 # 内部节点数量\n",
    "n_i = 5000  # 边界节点数量\n",
    "X_u_train = np.random.uniform(low=0, high=1, size=(n_u, 2))\n",
    "u_train = u_exact(X_u_train[:, 0:1], X_u_train[:, 1:2])\n",
    "X_i_train_1 = np.random.uniform(low=0, high=1, size=(1250, 2))\n",
    "X_i_train_1[:, 0] = 1\n",
    "X_i_train_2 = np.random.uniform(low=0, high=1, size=(1250, 2))\n",
    "X_i_train_2[:, 0] = 0\n",
    "X_i_train_3 = np.random.uniform(low=0, high=1, size=(1250, 2))\n",
    "X_i_train_3[:, 1] = 1\n",
    "X_i_train_4 = np.random.uniform(low=0, high=1, size=(1250, 2))\n",
    "X_i_train_4[:, 1] = 0\n",
    "X_i_train = np.concatenate((X_i_train_1, X_i_train_2, X_i_train_3, X_i_train_4), axis=0)\n",
    "X_i_train = tf.convert_to_tensor(X_i_train, dtype=tf.float64)\n",
    "# 创建 PINN 模型并进行训练\n",
    "Layers = [2, 40, 40,40,1]  # 神经网络层结构\n",
    "print(X_u_train)\n",
    "model = HeatPINN(Layers)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005))\n",
    "model.train(X_u_train, u_train, X_i_train, epochs=1000)\n",
    "n_pred = 5000  # 预测集中的点数量\n",
    "\n",
    "x_pred = np.random.uniform(low=0, high=1, size=(n_pred, 2))\n",
    "\n",
    "# 将预测集转换为 tf.Tensor 类型\n",
    "x_pred_tf = tf.convert_to_tensor(x_pred, dtype=tf.float64)\n",
    "\n",
    "# 使用模型预测\n",
    "y_pred = model.predict(x_pred_tf)\n",
    "\n",
    "# 将预测结果转换为 numpy 数组类型（如果必要）\n",
    "file_path =\"D:\\Desktop\\graduate paper\\hit pinn data_pred.npz\"\n",
    "np.savez(file_path, x_pred=x_pred, y_pred=y_pred)\n",
    "                  #对于每个训练点，您可以通过为其分配一个权重来实现点点自适应性方法。在训练过程中，可以根据每个训练点的损失值来调整权重。较高的损失值对应着较高的权重。\n",
    "\n",
    "#上述示例中，我添加了两个新的变量 `u_weights` 和 `f_weights`，它们分别用于存储内部节点和边界节点的权重。这些权重在每个训练步骤中都会被更新。\n",
    "\n",
    "#在 `train_step` 方法中，我将 `u_weights` 和 `f_weights` 作为参数传递给 `loss_U` 和 `loss_PDE` 方法。这些方法会根据权重计算相应的损失值。\n",
    "\n",
    "#在每个训练步骤的末尾，我使用 `tape.gradient()` 方法计算总损失相对于权重的梯度。然后，通过取梯度的绝对值并正则化，将权重归一化为和为1。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9f8b1-d4fd-49c3-b433-1bef2cb52ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
